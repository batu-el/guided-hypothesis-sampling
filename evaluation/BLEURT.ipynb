{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "# DATA_NAME = 'DEEN'\n",
    "# DATA_NAME = 'ENDE'\n",
    "# DATA_NAME = 'ENTR'\n",
    "DATA_NAME = 'TREN'\n",
    "DATA_PATH = f'data/data-merge/{DATA_NAME}.jsonl'\n",
    "DATA = []\n",
    "with jsonlines.open(DATA_PATH) as f:\n",
    "    for line in f.iter():\n",
    "        DATA.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = 1000\n",
    "reduced_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoded Data\n",
    "enc_candidates = []\n",
    "enc_candidates10 = []\n",
    "# Scores\n",
    "BLEURT_cand_scores = []\n",
    "CHRF_cand_scores = []\n",
    "# chrF++ Scores\n",
    "MBR_BLEURT_cand_scores = []\n",
    "MBR_CHRF_cand_scores = []\n",
    "# Winner Indexes\n",
    "MBR_BLEURT_winner_index = []\n",
    "MBR_CHRF_winner_index = []\n",
    "# Blackbox Functions\n",
    "enc10_to_idx = []\n",
    "enc10_to_MBR_BLEURT = []\n",
    "enc10_to_MBR_chrF = []\n",
    "# Rankings\n",
    "MBR_BLEURT_index_rank = []\n",
    "MBR_chrF_index_rank = []\n",
    "\n",
    "for idx in tqdm.tqdm(range(num_datapoints)):\n",
    "    idx_datapoint = DATA[idx]\n",
    "\n",
    "    # Encoded Data\n",
    "    _enc_data = np.array(idx_datapoint['enc_candidates'])\n",
    "    enc_candidates.append(_enc_data)\n",
    "    pca = PCA()\n",
    "    _enc_data_10 = pca.fit_transform(_enc_data)[:,:reduced_dim]\n",
    "    enc_candidates10.append(_enc_data_10)\n",
    "\n",
    "    # Scores\n",
    "    _bleurt_scores = np.array([np.round(score, 6)-0.5 for score in idx_datapoint['BLEURT_cand_scores']])\n",
    "    _chrf_scores = np.array([np.round(score, 6)-50 for score in idx_datapoint['CHRF_cand_scores']])\n",
    "    BLEURT_cand_scores.append(_bleurt_scores)\n",
    "    CHRF_cand_scores.append(_chrf_scores)\n",
    "\n",
    "    # MBR Scores\n",
    "    _mbr_bleurt_scores = np.array([np.round(score, 6) for score in idx_datapoint['MBR_BLEURT_cand_scores']])/128 -0.5\n",
    "    _mbr_chrf_scores = np.array([np.round(score, 6) for score in idx_datapoint['MBR_CHRF_cand_scores']])/128 -50\n",
    "    MBR_BLEURT_cand_scores.append(_mbr_bleurt_scores)\n",
    "    MBR_CHRF_cand_scores.append(_mbr_chrf_scores)\n",
    "\n",
    "    # Winner Indexes\n",
    "    _mbr_bleurt_winner_idx = idx_datapoint['MBR_BLEURT_winner_index']\n",
    "    _mbr_chrf_winner_idx = idx_datapoint['MBR_CHRF_winner_index']\n",
    "    MBR_BLEURT_winner_index.append(_mbr_bleurt_winner_idx)\n",
    "    MBR_CHRF_winner_index.append(_mbr_chrf_winner_idx)\n",
    "\n",
    "    # Blackbox MBR Function\n",
    "    _enc10_to_idx = {str(_enc_data_10[i]): i for i in range(128)}\n",
    "    _enc10_to_MBR_BLEURT = {str(enc10):_mbr_bleurt_scores[_enc10_to_idx[str(enc10)]] for enc10 in _enc10_to_idx.keys()}\n",
    "    _enc10_to_MBR_chrF = {str(enc10):_mbr_chrf_scores[_enc10_to_idx[str(enc10)]] for enc10 in _enc10_to_idx.keys()}\n",
    "\n",
    "    enc10_to_idx.append(_enc10_to_idx)\n",
    "    enc10_to_MBR_BLEURT.append(_enc10_to_MBR_BLEURT)\n",
    "    enc10_to_MBR_chrF.append(_enc10_to_MBR_chrF)\n",
    "\n",
    "    # Rankings\n",
    "    _MBR_BLEURT_index_rank = ss.rankdata(_mbr_bleurt_scores)\n",
    "    _MBR_chrF_index_rank = ss.rankdata(_mbr_chrf_scores)\n",
    "    MBR_BLEURT_index_rank.append(_MBR_BLEURT_index_rank)\n",
    "    MBR_chrF_index_rank.append(_MBR_chrF_index_rank)\n",
    "\n",
    "enc_candidates = np.array(enc_candidates)\n",
    "enc_candidates10 = np.array(enc_candidates10)\n",
    "BLEURT_cand_scores = np.array(BLEURT_cand_scores)\n",
    "CHRF_cand_scores = np.array(CHRF_cand_scores)\n",
    "MBR_BLEURT_cand_scores = np.array(MBR_BLEURT_cand_scores)\n",
    "MBR_CHRF_cand_scores = np.array(MBR_CHRF_cand_scores)\n",
    "MBR_BLEURT_winner_index = np.array(MBR_BLEURT_winner_index)\n",
    "MBR_CHRF_winner_index = np.array(MBR_CHRF_winner_index)\n",
    "### Blackbox Function Example Usage\n",
    "# enc10_to_MBR_chrF[0][str(enc_candidates10[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.space import Space\n",
    "from skopt.acquisition import gaussian_ei\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_best_MBR_BLEURT = []\n",
    "BASELINE_best_MBR_BLEURT_rank = []\n",
    "BASELINE_best_BLEURT = []\n",
    "\n",
    "MODEL_best_MBR_BLEURT = []\n",
    "MODEL_best_MBR_BLEURT_rank = []\n",
    "MODEL_best_BLEURT = []\n",
    "\n",
    "for experiment in range(10):\n",
    "    for idx in tqdm.tqdm(range(1000)):\n",
    "        # BLEURT\n",
    "        X = enc_candidates10[idx]\n",
    "        y = MBR_BLEURT_cand_scores[idx]\n",
    "        f = enc10_to_MBR_BLEURT[idx]\n",
    "        #######################\n",
    "        ### Random Baseline ###\n",
    "        #######################\n",
    "        n_samples = 10\n",
    "        initial_indices = np.random.choice(range(len(X)), n_samples, replace=False)\n",
    "        initial_samples = X[initial_indices]\n",
    "        initial_evaluations = [f[str(x)] for x in initial_samples]\n",
    "        best_index_local = np.argmax(initial_evaluations)\n",
    "        best_index = initial_indices[best_index_local]\n",
    "        # Results\n",
    "        best_MBR_BLEURT = MBR_BLEURT_cand_scores[idx][best_index]\n",
    "        best_MBR_BLEURT_rank = MBR_BLEURT_index_rank[idx][best_index]\n",
    "        best_BLEURT = BLEURT_cand_scores[idx][best_index]\n",
    "        # print(\"Baseline - best_MBR_BLEURT:\", best_MBR_BLEURT)\n",
    "        # print(\"Baseline - best_MBR_BLEURT_rank:\", best_MBR_BLEURT_rank)\n",
    "        # print(\"Baseline - best_BLEURT:\", best_BLEURT)\n",
    "        BASELINE_best_MBR_BLEURT.append(best_MBR_BLEURT)\n",
    "        BASELINE_best_MBR_BLEURT_rank.append(best_MBR_BLEURT_rank)\n",
    "        BASELINE_best_BLEURT.append(best_BLEURT)\n",
    "        #####################################################################################\n",
    "        ### Bayesian Optimization with Gaussian Process Regression - Expected Improvement ###\n",
    "        #####################################################################################\n",
    "        n_initial_points = 5\n",
    "        num_iterations = 5\n",
    "        initial_indices = np.random.choice(range(len(X)), n_initial_points, replace=False)\n",
    "        initial_samples = X[initial_indices]\n",
    "        initial_evaluations = [f[str(x)] for x in initial_samples]\n",
    "        # GP model\n",
    "        kernel = ConstantKernel(constant_value=1.0) * RBF(length_scale=1.0)\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5 , normalize_y =False,  alpha=0.00001)\n",
    "        # Train GP Model\n",
    "        gp.fit(initial_samples, initial_evaluations)\n",
    "\n",
    "        for _ in range(num_iterations):\n",
    "            # Current best\n",
    "            current_best = np.max(gp.y_train_)\n",
    "            # Acquisition values\n",
    "            acquisition_values = - gaussian_ei(X, gp, y_opt=current_best, xi=0.01)\n",
    "            # acquisition_values = np.zeros([128]) # Random Baseline\n",
    "            # Filter out previously evaluated points\n",
    "            evaluated_points = set(map(tuple, gp.X_train_))\n",
    "            filtered_acquisition = np.array([acq if tuple(point) not in evaluated_points else -np.inf for point, acq in zip(X, acquisition_values)])\n",
    "            # Select the next point\n",
    "            next_index = np.argmax(filtered_acquisition)\n",
    "            next_point = X[next_index]\n",
    "            # Evaluate the next point\n",
    "            next_evaluation = f[str(next_point)]\n",
    "            # Train GP model including the new point\n",
    "            X_new = np.vstack([gp.X_train_, next_point])\n",
    "            Y_new = np.append(gp.y_train_, next_evaluation)\n",
    "            gp.fit(X_new,Y_new)\n",
    "\n",
    "        # Results\n",
    "        best_evaluation_index_local = np.argmax(gp.y_train_)\n",
    "        best_point = gp.X_train_[best_evaluation_index_local]\n",
    "        best_value = gp.y_train_[best_evaluation_index_local]\n",
    "        best_evaluation_index = enc10_to_idx[idx][str(best_point)]\n",
    "        best_MBR_BLEURT = MBR_BLEURT_cand_scores[idx][best_evaluation_index]\n",
    "        best_MBR_BLEURT_rank = MBR_BLEURT_index_rank[idx][best_evaluation_index]\n",
    "        best_BLEURT = BLEURT_cand_scores[idx][best_evaluation_index]\n",
    "        # print(\"Model best_MBR_BLEURT:\", best_MBR_BLEURT)\n",
    "        # print(\"Model best_MBR_BLEURT_rank:\", best_MBR_BLEURT_rank)\n",
    "        # print(\"Model best_BLEURT:\", best_BLEURT)\n",
    "        MODEL_best_MBR_BLEURT.append(best_MBR_BLEURT)\n",
    "        MODEL_best_MBR_BLEURT_rank.append(best_MBR_BLEURT_rank)\n",
    "        MODEL_best_BLEURT.append(best_BLEURT)\n",
    "        # print('---')\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BASELINE_best_MBR_BLEURT:',np.mean(BASELINE_best_MBR_BLEURT)+0.5)\n",
    "print('BASELINE_best_MBR_BLEURT_rank:',np.mean(BASELINE_best_MBR_BLEURT_rank))\n",
    "print('BASELINE_best_BLEURT:',np.mean(BASELINE_best_BLEURT)+0.5)\n",
    "\n",
    "print('MODEL_best_MBR_BLEURT:',np.mean(MODEL_best_MBR_BLEURT)+0.5)\n",
    "print('MODEL_best_MBR_BLEURT_rank:',np.mean(MODEL_best_MBR_BLEURT_rank))\n",
    "print('MODEL_best_BLEURT:',np.mean(MODEL_best_BLEURT)+0.5)\n",
    "\n",
    "print('UPPER_BOUND MBR BLEURT:', np.array([MBR_BLEURT_cand_scores[idx][MBR_BLEURT_winner_index[idx]] for idx in range(num_datapoints)]).mean()+0.5)\n",
    "print('UPPER_BOUND RANK:', np.array([MBR_BLEURT_index_rank[idx][MBR_BLEURT_winner_index[idx]]for idx in range(num_datapoints)]).mean())\n",
    "print('UPPER_BOUND BLEURT:', np.array([BLEURT_cand_scores[idx][MBR_BLEURT_winner_index[idx]]for idx in range(num_datapoints)]).mean()+0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(BASELINE_best_MBR_BLEURT_rank)\n",
    "print(n)\n",
    "print('Baseline is Better', sum([BASELINE_best_MBR_BLEURT_rank[i] > MODEL_best_MBR_BLEURT_rank[i] for i in range(n)]))\n",
    "print('Model is Better', sum([BASELINE_best_MBR_BLEURT_rank[i] < MODEL_best_MBR_BLEURT_rank[i] for i in range(n)]))\n",
    "print('Tie', sum([BASELINE_best_MBR_BLEURT_rank[i] == MODEL_best_MBR_BLEURT_rank[i] for i in range(n)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1 = np.array(BASELINE_best_MBR_BLEURT)\n",
    "val2 = np.array(MODEL_best_MBR_BLEURT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1\n",
    "sns.set(style=\"whitegrid\")\n",
    "bins = np.histogram_bin_edges(np.concatenate((val1, val2)), bins=30)\n",
    "plt.hist([val1, val2], bins=bins, color=['blue', 'red'], edgecolor='black', label=['Baseline', 'Model'], alpha=0.7)\n",
    "plt.xlabel('Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'{DATA_NAME}', fontsize=22)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2\n",
    "plt.figure(figsize=(12, 8))  \n",
    "plt.scatter(val1, val2, alpha=0.6, edgecolors='w', s=80) \n",
    "plt.plot([min(min(val1),min(val2)) , max(max(val1),max(val2))], [min(min(val1),min(val2)), max(max(val1),max(val2))], color='red', linewidth=2)  # Assuming the values in val1 and val2 are between 0 and 1\n",
    "plt.xlabel('Baseline', fontsize=22)\n",
    "plt.ylabel('Model', fontsize=22)\n",
    "plt.title(f'{DATA_NAME}', fontsize=22)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
